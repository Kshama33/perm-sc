{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c25c9e83-a4f8-479a-8d74-41aa6cc0c521",
   "metadata": {},
   "source": [
    "# Sorting Tasks (GPT-3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660e1149-6ca8-4d56-a937-5b96b9c3627b",
   "metadata": {},
   "source": [
    "**Global Inputs**\n",
    "- Set your OpenAI API key here. If you're using Azure, see the code documentation for `OpenAIConfig` for how to modify it.\n",
    "- Set the aggregate size here. The default of 5 works well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41cf3fb-f47e-4c0c-835c-079365b29a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = ''\n",
    "api_type = 'openai'\n",
    "num_aggregates = 5  # number of aggregates\n",
    "num_limit = 50  # set to 100 for the full dataset; 50 is for fast inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc7a6d6-1243-4f60-b64d-eef8270b2967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from permsc import SentenceSortRankingPromptBuilder, FastMathSortRankingPromptBuilder, OpenAIPromptPipeline, OpenAIConfig, ChatCompletionPool\n",
    "\n",
    "config = OpenAIConfig(model_name='gpt-3.5-turbo', api_key=api_key, api_type=api_type)\n",
    "builder = FastMathSortRankingPromptBuilder()\n",
    "pool = ChatCompletionPool([config] * 5)  # 5 parallel instances\n",
    "pipeline = OpenAIPromptPipeline(builder, pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6895e8c1-8ee3-4440-b1ef-72c3dc0711b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def run_pipeline(pipeline, dataset, num_aggregates, limit=100):\n",
    "    prefs_list = []\n",
    "    perms_list = []\n",
    "    gts_list = []\n",
    "    \n",
    "    for it in dataset[:limit]:\n",
    "        it = deepcopy(it)\n",
    "        gt_perm = it.randomize_order(standardize=True)\n",
    "        gts_list.append(gt_perm)\n",
    "        prefs = []\n",
    "        items = []\n",
    "        perms = []\n",
    "        \n",
    "        for _ in range(num_aggregates):\n",
    "            it_cpy = deepcopy(it)\n",
    "            perms.append(it_cpy.randomize_order())\n",
    "            items.append(it_cpy)\n",
    "    \n",
    "        outputs = pipeline.run(items, temperature=0, request_timeout=10)\n",
    "    \n",
    "        for output, perm in zip(outputs, perms):\n",
    "            pref_restore_map = dict(zip(range(len(perm)), perm))\n",
    "            pref_restore_map[-1] = -1\n",
    "            prefs.append(np.array([pref_restore_map[x] for x in output]))\n",
    "    \n",
    "        prefs_list.append(np.array(prefs))\n",
    "        perms_list.append(np.array(perms))\n",
    "\n",
    "    return prefs_list, perms_list, gts_list  # output permutations, input permutations, and ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb30a6d6-fb89-4a06-8dd8-466ccc1be276",
   "metadata": {},
   "outputs": [],
   "source": [
    "from permsc import KemenyOptimalAggregator\n",
    "\n",
    "def aggregate(prefs_list):\n",
    "    aggregator = KemenyOptimalAggregator()\n",
    "    results = []\n",
    "\n",
    "    for prefs in prefs_list:\n",
    "        results.append(aggregator.aggregate(prefs))\n",
    "\n",
    "    return results\n",
    "\n",
    "def compute_individual_taus(prefs_list, gts_list):\n",
    "    taus = []\n",
    "\n",
    "    for idx in range(num_aggregates):\n",
    "        tau = []\n",
    "        \n",
    "        for gt, prefs in zip(gts_list, prefs_list):\n",
    "            if len(prefs) > 0:\n",
    "                tau.append(fn(np.argsort(gt), prefs[idx]))\n",
    "    \n",
    "        taus.append(np.mean(tau))\n",
    "    \n",
    "    return np.array(taus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b840d78-c376-4dde-af17-423563d4d9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from permsc import ranks_from_preferences\n",
    "import scipy.stats as stats\n",
    "\n",
    "fn = lambda x, y: stats.kendalltau(ranks_from_preferences(x), ranks_from_preferences(y))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71c306b-9413-4dc9-888e-74b2ae9f9583",
   "metadata": {},
   "source": [
    "## MathSort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a70b6e-dc8c-4dd4-abfd-3d93fd0e0ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from permsc import MathSortDataset\n",
    "\n",
    "ds = MathSortDataset('../data/mathsort.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222b9d13-cef6-453d-9f6d-0bb4c238c04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefs_list, _, gts_list = run_pipeline(pipeline, ds, num_aggregates, limit=num_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2ffb08-f63e-4272-b690-112fb6857229",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = aggregate(prefs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95d6764-3597-43a9-80c4-ea4b1c76ab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggr_score = np.mean([fn(np.argsort(gt), x) for gt, x in zip(gts_list, results)])\n",
    "aggr_score  # Aggregate tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1a1fa7-e1f5-46a1-ae41-9b8be767524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_individual_taus(prefs_list, gts_list)  # Individual runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a274d6-e483-4855-9e5a-a7fe3068630d",
   "metadata": {},
   "source": [
    "## WordSort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e390e9-6d6b-499d-a3eb-2392002e11a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from permsc import WordSortRankingPromptBuilder, WordSortDataset\n",
    "\n",
    "ds = WordSortDataset('../data/wordsort.tsv')\n",
    "builder = WordSortRankingPromptBuilder()\n",
    "pool = ChatCompletionPool([config] * 5)  # 5 parallel instances\n",
    "pipeline = OpenAIPromptPipeline(builder, pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94836ae7-3205-4a6f-ad70-723921bb2658",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefs_list, _, gts_list = run_pipeline(pipeline, ds, num_aggregates, limit=num_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753c6b4f-9bc7-4aab-ba40-155ed2cbe981",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = aggregate(prefs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11955d1b-0910-49ba-8112-c150e8b072e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggr_score = np.mean([fn(np.argsort(gt), x) for gt, x in zip(gts_list, results)])\n",
    "aggr_score  # Aggregate tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f549ac-aeee-44bc-8c1e-cd5a1aece1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_individual_taus(prefs_list, gts_list)  # Individual runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daee536-d8f9-460e-8777-c75acb39166e",
   "metadata": {},
   "source": [
    "## GSM8KSort\n",
    "- Please use Azure with this, as OpenAI seems to break."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a9908d-fe50-4be7-b3eb-85993d57b519",
   "metadata": {},
   "outputs": [],
   "source": [
    "from permsc import GSM8KSortDataset, SentenceSortRankingPromptBuilder\n",
    "\n",
    "ds = GSM8KSortDataset('../data/gsm8ksort.jsonl')\n",
    "builder = SentenceSortRankingPromptBuilder()\n",
    "pool = ChatCompletionPool([config] * 5)  # 5 parallel instances\n",
    "pipeline = OpenAIPromptPipeline(builder, pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3cfda2-3241-45a2-8e47-dca1c5240906",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefs_list, _, gts_list = run_pipeline(pipeline, ds, num_aggregates, limit=num_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6fa59b-886e-431b-b16d-54d9951ad2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = aggregate(prefs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff653c1-6912-425f-85d0-b7ef9cb47efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggr_score = np.mean([fn(np.argsort(gt), x) for gt, x in zip(gts_list, results)])\n",
    "aggr_score  # Aggregate tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dbd8bf-9b7c-43bd-8a8f-f962a579a461",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_individual_taus(prefs_list, gts_list)  # Individual runs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
